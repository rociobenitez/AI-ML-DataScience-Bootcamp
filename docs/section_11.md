# Deep Learning (en proceso)

## √çndice

1. [¬øQu√© es TensorFlow?](#qu√©-es-tensorflow)
2. [¬øQu√© es una GPU?](#qu√©-es-una-gpu)
3. [Eligiendo un modelo](#eligiendo-un-modelo-throwback)
4. [¬øQu√© es Deep Learning?](#qu√©-es-deep-learning)
5. [¬øQu√© son las Redes Neuronales?](#qu√©-son-las-redes-neuronales)
6. [Tipos de problemas de Deep Learning](#tipos-de-problemas-de-deep-learning)
7. [¬øQu√© es el aprendizaje por transferencia?](#qu√©-es-el-aprendizaje-por-transferencia-transfer-learning)
8. [Google Colab](#google-colab)
9. [Obtenci√≥n de Datos](#obtenci√≥n-de-datos)
10. [Exploraci√≥n de los Datos](#exploraci√≥n-de-los-datos)
11. [Procesamiento de Datos](#procesamiento-de-datos)
12. [Procesamiento de Im√°genes](#procesamiento-de-im√°genes)
13. [Construyendo un Modelo de Deep Learning](#construyendo-un-modelo-de-deep-learning)
14. [Entrenamiento y Evaluaci√≥n](#entrenamiento-y-evaluaci√≥n)
15. [Predicciones y Visualizaci√≥n](#predicciones-y-visualizaci√≥n)
16. [Guardar y Cargar Modelos](#guardar-y-cargar-modelos)
17. [Ajustar el modelo al dataset completo](#ajustar-el-modelo-al-dataset-completo)
18. [Enviar el Modelo a Kaggle](#enviar-el-modelo-a-kaggle)

## ¬øQu√© es TensorFlow?

[TensorFlow](https://www.tensorflow.org/?hl=es) es un marco de trabajo de c√≥digo abierto desarrollado por Google que permite **construir, entrenar y desplegar modelos de machine learning y deep learning**. Su versatilidad y escalabilidad lo convierten en una herramienta popular tanto en investigaci√≥n como en aplicaciones comerciales.

Dentro de TensorFlow, tambi√©n puedes usar [Keras](https://www.tensorflow.org/guide/keras?hl=es-419), un marco complementario conocido por su facilidad de uso y simplicidad.

### Caracter√≠sticas principales

- Soporta operaciones en CPU y GPU para un entrenamiento r√°pido.
- Facilita la construcci√≥n de modelos avanzados como redes neuronales convolucionales y recurrentes.
- Integraci√≥n con [TensorFlow Hub](https://www.tensorflow.org/hub?hl=es) para usar modelos preentrenados.
- Dise√±ado para implementaciones escalables en servidores, dispositivos m√≥viles y la nube.

> - üîó [Tutoriales de TensorFlow](https://www.tensorflow.org/tutorials?hl=es)
> - üîó [Gu√≠a TensorFlow](https://www.tensorflow.org/guide?hl=es)
> - üîó [Recursos educativos TensorFlow](https://www.tensorflow.org/resources/learn-ml?hl=es)

### ¬øPor qu√© usar TensorFlow?

TensorFlow te permite manipular datos y escribir algoritmos de aprendizaje profundo usando c√≥digo Python. Adem√°s, cuenta con capacidades integradas para aprovechar hardware acelerado como GPUs (unidades de procesamiento gr√°fico) y TPUs (unidades de procesamiento tensorial). Muchas de las compa√±√≠as m√°s grandes del mundo [impulsan sus cargas de trabajo de aprendizaje autom√°tico con TensorFlow](https://www.tensorflow.org/about/case-studies?hl=es-419).

**Ventajas**

1. **Escritura de c√≥digo r√°pido:** Compatible con Python y optimizado para GPUs.
2. **Acceso a modelos preentrenados:** La biblioteca [TensorFlow Hub](https://www.tensorflow.org/hub?hl=es) ofrece una amplia gama de modelos listos para usar.
3. **Pila completa:** Permite manejar todo el ciclo de vida del modelo, desde el preprocesamiento de datos hasta el despliegue.
4. **Respaldo de Google:** Originalmente dise√±ado y usado internamente por Google, ahora es una herramienta de c√≥digo abierto ampliamente adoptada.

> üîó [¬øPor qu√© TensorFlow?](https://www.tensorflow.org/about?hl=es)

## ¬øQu√© es una GPU?

Una [GPU (Graphics Processing Unit)](https://en.wikipedia.org/wiki/Graphics_processing_unit) es un procesador especializado en realizar c√°lculos matem√°ticos en paralelo, lo que la hace ideal para entrenar modelos de deep learning que requieren operaciones matriciales intensivas.

> üîó [Unidad de Procesamiento gr√°fico](https://es.wikipedia.org/wiki/Unidad_de_procesamiento_gr%C3%A1fico)

**Ventajas en Deep Learning**

- **Entrenamiento r√°pido:** Las GPUs pueden procesar grandes cantidades de datos simult√°neamente.
- **Eficiencia en redes neuronales:** Optimizadas para manejar operaciones con tensores, comunes en deep learning.

## Eligiendo un modelo (Throwback)

Seleccionar el modelo adecuado depende del tipo de problema y los datos disponibles.

### Problemas con datos estructurados (tablas)

Este tipo de datos se organiza en forma tabular, con **columnas que representan caracter√≠sticas (features) y filas que representan instancias**.

**Modelos recomendados:**

- **CatBoost:**
  - Especialmente bueno para datos categ√≥ricos y tabulares con relaciones complejas.
  - Es eficiente, incluso sin un preprocesamiento extenso.
- **XGBoost:**
  - Modelo basado en √°rboles de decisi√≥n, ideal para problemas estructurados.
  - Es potente para tareas de clasificaci√≥n y regresi√≥n, siendo r√°pido y escalable.
- **Random Forest:**
  - Combina m√∫ltiples √°rboles de decisi√≥n entrenados en subconjuntos aleatorios de datos para mejorar la precisi√≥n y evitar sobreajuste.

### Problemas con datos no estructurados (im√°genes, texto, audio)

Este tipo de datos **no sigue una estructura definida**, como fotograf√≠as, clips de audio o documentos de texto.

**Modelos recomendados:**

- **Redes neuronales profundas:** Utilizan arquitecturas como redes convolucionales (CNN) para im√°genes, o redes recurrentes (RNN) para secuencias como texto o audio. Pueden aprender representaciones jer√°rquicas directamente de los datos.
- **Transfer learning:** Consiste en usar un modelo preentrenado en un gran conjunto de datos (como ImageNet) y adaptarlo a tu problema espec√≠fico. Esto es √∫til cuando tienes pocos datos.
- **TensorFlow Hub:** Biblioteca que proporciona modelos preentrenados listos para ser reutilizados, como clasificadores de im√°genes o modelos de procesamiento de lenguaje natural.

## ¬øQu√© es Deep Learning?

[Deep Learning](https://en.wikipedia.org/wiki/Deep_learning) es una rama del machine learning que utiliza [redes neuronales profundas](<https://en.wikipedia.org/wiki/Neural_network_(machine_learning)>) para aprender patrones complejos en los datos. Es especialmente √∫til en aplicaciones como reconocimiento de im√°genes, procesamiento de lenguaje natural y sistemas de recomendaci√≥n.

<img src="../assets/section-11/IA-Machine-Learning-y-Deep-Learning.png" alt="IA vs ML vs Deep learning" width="500" style="margin: 24px auto; background: white;">

**Ejemplo pr√°ctico:** Una foto puede convertirse en n√∫meros (valores de p√≠xeles rojo, verde y azul) y esos n√∫meros se manipulan matem√°ticamente a trav√©s de cada capa para aprender patrones en la imagen.

**Ventajas:**

- Capacidad de trabajar con grandes vol√∫menes de datos no estructurados.
- Automatiza la ingenier√≠a de caracter√≠sticas al aprender directamente de los datos.

<img src="../assets/section-11/ML-vs-DL.webp" alt="ML vs Deep learning" width="500" style="margin: 24px auto; background: white;">

**T√©rminos importantes:**

- **‚ÄúDeep‚Äù (profundo):** Proviene del n√∫mero de capas en la red neuronal.
- **Relaci√≥n con AI y ML:**
  - Inteligencia artificial (IA): Tecnolog√≠a general.
  - Aprendizaje autom√°tico (ML): Subcategor√≠a de IA.
  - Aprendizaje profundo (DL): Subcategor√≠a de ML.

#### ¬øPara qu√© se utiliza el aprendizaje profundo?

El aprendizaje profundo alimenta la mayor√≠a de las aplicaciones modernas de inteligencia artificial. Algunos ejemplos incluyen:

- **AI predictiva:** Como modelos que aprenden relaciones entre datos (e.g., fotos de perros y sus razas) para hacer predicciones futuras.
- **[AI generativa](https://en.wikipedia.org/wiki/Generative_artificial_intelligence):** Que crea algo nuevo, como texto o im√°genes, a partir de un input.

**Ejemplos de AI predictiva:**

- [Coches aut√≥nomos de Tesla](https://www.tesla.com/AI) que detectan objetos en la carretera.
- [Reconocimiento facial](https://machinelearning.apple.com/research/recognizing-people-photos) en aplicaciones como Apple Photos.
- Asistentes de voz como Siri o Google Assistant.
- [Nutrify](https://nutrify.app/) utiliza IA predictiva para reconocer alimentos en im√°genes.
- [Magika](https://google.github.io/magika/) utiliza aprendizaje profundo para clasificar un archivo seg√∫n su tipo (por ejemplo, .jpeg, .py, .txt).
- [Los modelos de clasificaci√≥n de texto](https://huggingface.co/MoritzLaurer/deberta-v3-large-zeroshot-v2.0) como DeBERTa utilizan aprendizaje profundo para clasificar texto en diferentes categor√≠as como "positivo" y "negativo" o "spam" o "no spam".

**Ejemplos de AI generativa:**

- [Stable Diffusion](https://huggingface.co/spaces/stabilityai/stable-diffusion) utiliza inteligencia artificial generativa para generar im√°genes a partir de un mensaje de texto.
- [ChatGPT](https://chatgpt.com/) y otros modelos de lenguaje grandes (LLMs) como Llama, Claude, Gemini y Mistral utilizan aprendizaje profundo para procesar texto y devolver una respuesta.
- [GitHub Copilot](https://github.com/features/copilot) utiliza inteligencia artificial generativa para generar fragmentos de c√≥digo a partir del contexto circundante.

## ¬øQu√© son las Redes Neuronales?

Las redes neuronales son modelos computacionales inspirados en la estructura y funci√≥n del cerebro humano. Est√°n formadas por nodos (neuronas) organizados en capas.

### Componentes clave

- **Capas (Layers):**
  - Entrada (Input layers): Recibe los datos iniciales.
  - Ocultas (Hidden layers): Procesan los datos para extraer patrones.
  - Salida (Output Layers): Genera las predicciones.
- **Funciones de activaci√≥n:** Introducen no linealidades, permitiendo a la red aprender patrones complejos.

<img src="../assets/section-11/neural-network.png" alt="Redes neuronales" width="500" style="margin: 24px auto; background: white;">

## Tipos de problemas de Deep Learning

El aprendizaje profundo aborda una variedad de problemas. Aqu√≠ presentamos los tipos m√°s comunes y su enfoque pr√°ctico:

### 1. Clasificaci√≥n (Classification)

- **Descripci√≥n:** Asignar una etiqueta a una entrada. Ejemplo: Clasificar im√°genes como "perro" o "gato".
- **Aplicaciones:** Detecci√≥n de spam, reconocimiento de objetos, diagn√≥stico m√©dico.
- **Modelos t√≠picos:** Redes neuronales profundas (DNN), redes convolucionales (CNN).

### 2. Regresi√≥n (Regression)

- **Descripci√≥n:** Predecir un valor num√©rico continuo. Ejemplo: Estimar el precio de una casa basado en su tama√±o.
- **Aplicaciones:** Predicci√≥n de series temporales, an√°lisis financiero, predicci√≥n de clima.
- **Modelos t√≠picos:** Redes neuronales profundas con funciones de activaci√≥n lineales.

### 3. Detecci√≥n de objetos (Object Detection)

- **Descripci√≥n:** Identificar y localizar objetos en im√°genes o videos.
- **Aplicaciones:** Sistemas de seguridad, veh√≠culos aut√≥nomos, monitoreo de tr√°fico.
- **Modelos t√≠picos:** YOLO, Faster R-CNN, SSD.

### 4. Segmentaci√≥n de im√°genes (Image Segmentation)

- **Descripci√≥n:** Dividir una imagen en regiones significativas. Ejemplo: Delimitar un tumor en una resonancia magn√©tica.
- **Aplicaciones:** Medicina, mapeo satelital, an√°lisis de texto manuscrito.
- **Modelos t√≠picos:** U-Net, Mask R-CNN.

### 5. Procesamiento de lenguaje natural (NLP)

- **Descripci√≥n:** Trabajar con texto o lenguaje hablado. Ejemplo: An√°lisis de sentimientos, traducci√≥n autom√°tica.
- **Aplicaciones:** Chatbots, res√∫menes de texto, asistentes virtuales.
- **Modelos t√≠picos:** Transformers (BERT, GPT), LSTM, GRU.

### 6. Generaci√≥n de datos (Data Generation)

- **Descripci√≥n:** Crear nuevos datos similares a los existentes. Ejemplo: Generar im√°genes realistas a partir de ruido.
- **Aplicaciones:** Creaci√≥n de contenido, dise√±o asistido, simulaciones.
- **Modelos t√≠picos:** GANs (Generative Adversarial Networks), VAEs (Variational Autoencoders).

### 7. Sequence-to-Sequence (Seq2Seq)

- **Descripci√≥n:** Transformar una secuencia de entrada en otra secuencia. Ejemplo: Traducci√≥n de idiomas.
- **Aplicaciones:** Subtitulaci√≥n autom√°tica, conversi√≥n texto-voz.
- **Modelos t√≠picos:** LSTMs, GRUs, Transformers.

### 8. Aprendizaje por refuerzo (Reinforcement Learning)

- **Descripci√≥n:** Entrenar un agente para tomar decisiones √≥ptimas en un entorno. Ejemplo: Jugar videojuegos o controlar robots.
- **Aplicaciones:** Automatizaci√≥n, rob√≥tica, sistemas de recomendaci√≥n.
- **Modelos t√≠picos:** Deep Q-Learning, A3C, DDPG.

## ¬øQu√© es el aprendizaje por transferencia (transfer learning)?

El aprendizaje por transferencia, o **[transfer learning](https://en.wikipedia.org/wiki/Transfer_learning)**, es una t√©cnica en la que se utiliza un modelo preentrenado en una tarea similar (generalmente en un gran conjunto de datos) y se adapta para resolver un nuevo problema. Esto es especialmente √∫til cuando no tienes suficientes datos para entrenar un modelo desde cero.

<img src="../assets/section-11/transfer-learning.webp" alt="Diagrama del aprendizaje por transferencia" width="500" style="margin: 24px auto; background: white;">

Implica tomar lo que un modelo o red neuronal ha aprendido en un dominio similar y aplicarlo a tu propio problema.

Por ejemplo:

- Se pueden usar patrones que una red neuronal haya aprendido de m√°s de 1 mill√≥n de im√°genes y 1000 clases en el conjunto de datos [ImageNet](https://www.image-net.org/) (un est√°ndar de referencia en visi√≥n por computadora) y aplicarlos al problema de reconocer razas de perros.

<img src="../assets/section-11/unstructured-data-a-transfer-learning-workflow.png" alt="Fuljo aprendizaje por transferencia" width="700" style="margin: 16px auto; background: white;">

Este concepto tambi√©n es aplicable a otros dominios:

- Usar un modelo de lenguaje preentrenado (LLM) que haya aprendido patrones del lenguaje natural en la mayor√≠a de los textos disponibles en internet y personalizarlo para un caso espec√≠fico, como un chatbot.

**Beneficios principales del aprendizaje por transferencia:**

- **Resultados sobresalientes con menos datos y tiempo:** Permite obtener buenos resultados incluso con conjuntos de datos peque√±os.
- **Flexibilidad:** Los modelos preentrenados en grandes conjuntos de datos pueden adaptarse a una amplia gama de tareas.

### ¬øPor qu√© deber√≠amos usarlo?

1. **Ahorro de tiempo y recursos:**
   - Entrenar un modelo desde cero en grandes cantidades de datos puede ser costoso en t√©rminos de tiempo y computaci√≥n. Con transfer learning, el modelo ya ha aprendido caracter√≠sticas generales que pueden ser √∫tiles en la nueva tarea.
2. **Mejor desempe√±o con datos limitados:**

   - Cuando los datos disponibles son limitados, entrenar un modelo desde cero puede llevar a un sobreajuste. Al usar transfer learning, el modelo aprovecha el conocimiento adquirido previamente y mejora la generalizaci√≥n.

3. **Casos pr√°cticos comunes:**
   - Reconocimiento de im√°genes: Usar modelos como ResNet o VGG, preentrenados en el conjunto de datos ImageNet, para tareas espec√≠ficas como clasificaci√≥n m√©dica.
   - Procesamiento de lenguaje natural (NLP): Utilizar modelos como BERT o GPT entrenados en grandes corpus de texto para tareas como an√°lisis de sentimientos o traducci√≥n.

### ¬øC√≥mo funciona?

- **Congelar capas:** En muchos casos, se congelan las primeras capas del modelo preentrenado, que capturan caracter√≠sticas generales (como bordes y texturas en im√°genes), y solo se ajustan las √∫ltimas capas para adaptarlas a la tarea espec√≠fica.
- **[Fine-tuning](https://es.wikipedia.org/wiki/Ajuste_fino_(aprendizaje_profundo):** Otra opci√≥n es ajustar los pesos de todo el modelo (o parte de √©l) en el nuevo conjunto de datos para mejorar la precisi√≥n.

## Google Colab

Google Colab es una plataforma basada en la nube para ejecutar c√≥digo Python, especialmente √∫til en deep learning.

### Ventajas de Colab

- Acceso gratuito a GPUs y TPUs.
- No requiere configuraci√≥n local del entorno.
- Permite compartir notebooks f√°cilmente.

### Configuraci√≥n inicial

1. Crear un nuevo notebook en [Google Colab](https://colab.research.google.com/).
2. Configurar el uso de GPU en: `Entorno de ejecuci√≥n > Cambiar tipo de entorno de ejecuci√≥n > GPU`.

- Antes de ejecutar cualquier c√≥digo, nos aseguraremos de que nuestra instancia de Google Colab est√© conectada a una GPU.
- **¬øPor qu√© utilizar una GPU?** El entrenamiento de redes neuronales involucra una gran cantidad de c√°lculos matem√°ticos, principalmente [multiplicaciones de matrices](https://en.wikipedia.org/wiki/Matrix_multiplication). Una GPU (Graphics Processing Unit) es mucho m√°s eficiente que una CPU para realizar estas operaciones, lo que acelera considerablemente el tiempo de entrenamiento.

  <img src="../assets/section-11/gpu-google-colab.png" alt="Cambiar tipo de entorno de ejecuci√≥n en Google Colab" width="400" style="margin: 16px auto; background: white;">

> - üîó [Ejemplo de notebook en Google Colab](https://colab.research.google.com/notebooks/io.ipynb)
> - üîó [Introducci√≥n a Colaboratory](https://colab.research.google.com/notebooks/intro.ipynb)

## Obtenci√≥n de Datos

En cualquier proyecto de machine learning (y deep learning), todo comienza con los datos. Sin datos, no hay proyecto.

Existen diversas opciones y recursos donde podemos obtener datos para un proyecto de deep learning.

### Recursos comunes para obtener datasets

| Recurso                                                                 | Descripci√≥n                                                                                                                                                      |
| :---------------------------------------------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [**Kaggle Datasets**](https://www.kaggle.com/datasets)                  | Una colecci√≥n de conjuntos de datos sobre una amplia gama de temas.                                                                                              |
| [**TensorFlow Datasets**](https://www.tensorflow.org/datasets)          | Conjuntos de datos de machine learning listos para usar con la API tf.data.Datasets. Puedes ver la lista completa de datasets en la documentaci√≥n de TensorFlow. |
| [**Hugging Face Datasets**](https://huggingface.co/datasets)            | Un recurso en crecimiento continuo con datasets organizados en varias categor√≠as.                                                                                |
| [**Google Dataset Search**](https://datasetsearch.research.google.com/) | Un motor de b√∫squeda de Google enfocado espec√≠ficamente en buscar datasets en l√≠nea.                                                                             |
| **Fuentes originales**                                                  | Conjuntos de datos disponibles a trav√©s de investigadores o empresas, generalmente relacionados con un producto o art√≠culo de investigaci√≥n.                     |
| **Datasets personalizados**                                             | Conjuntos de datos creados a partir de tus propios datos. Por ejemplo, tu biblioteca de fotos, documentos personales o historial de pedidos de clientes.         |

### Datos preexistentes vs. datos personalizados

Cuando trabajamos con datasets existentes, como los mencionados anteriormente, estos suelen venir preformateados y listos para usar. Por otro lado, los datasets personalizados que creamos nosotros mismos o utilizamos en un entorno corporativo a menudo requieren preprocesamiento adicional antes de ser utilizados en un proyecto de machine learning.

**Diferencias clave:**

- **Datasets existentes:** A menudo vienen con divisiones de entrenamiento y prueba ya preparadas, lo que reduce el tiempo necesario para la preparaci√≥n.
- **Datasets personalizados:** Suelen requerir tareas como limpieza, etiquetado y divisi√≥n manual en conjuntos de datos.

### Consideraciones al usar Google Colab

Si utilizas Google Colab, recuerda que **los datos cargados en la sesi√≥n se eliminan si la conexi√≥n se interrumpe**. Para evitar descargar los datos cada vez que se reinicia la sesi√≥n, seguiremos estos pasos:

- Descargar los datos una vez desde el sitio web original.
- Guardar los datos en Google Drive, ya que Google Colab se integra f√°cilmente con este servicio.
- Verificar si los datos ya existen en Google Drive. Si est√°n disponibles, los importaremos directamente a la sesi√≥n de Colab.
- Si los datos no existen en Google Drive, los descargaremos desde el sitio web original y los copiaremos a Google Drive para uso futuro.

> [!NOTE]
> üìö **Recurso:** Para una buena gu√≠a sobre c√≥mo introducir y extraer datos en Google Colab, consulte el [tutorial de Google Colab io.ipynb](https://colab.research.google.com/notebooks/io.ipynb#scrollTo=RWSJpsyKqHjH).

### Conexi√≥n de Google Colab con Google Drive

Existen dos opciones principales para conectar Google Colab con Google Drive:

1. Hacer clic en **"Mount Drive"** desde el men√∫ **"Files"** en la lado izquierdo de Colab.
2. Montar program√°ticamente Google Drive con el siguiente c√≥digo:

```python
from google.colab import drive
drive.mount('/content/drive')
```

  <img src="../assets/section-11/connect-google-drive-colab.jpeg" alt="Sincronizar Google Drive con el cuaderno de Colab" width="400" style="background: white;">

### Flujo de trabajo para gestionar los datos del proyecto

M√°s espec√≠ficamente, seguiremos los siguientes pasos:

1. **Montar Google Drive:** Esto permitir√° guardar y recuperar archivos directamente desde tu unidad de Google Drive.
2. **Configurar constantes**: Definir la ubicaci√≥n base donde guardaremos los archivos, las URLs de descarga y los nombres de los archivos objetivos.

```
BASE_DIR = '/content/drive/My Drive/dog-breed-identification/'
FILES = {
    'images': 'images.tar',
    'annotations': 'annotation.tar',
    'lists': 'lists.tar'
}
DOWNLOAD_URLS = {
    'images': 'http://vision.stanford.edu/aditya86/ImageNetDogs/images.tar',
    'annotations': 'http://vision.stanford.edu/aditya86/ImageNetDogs/annotation.tar',
    'lists': 'http://vision.stanford.edu/aditya86/ImageNetDogs/lists.tar'
}
```

3. **Configurar la ruta local de destino**: Definir un directorio local dentro de Colab para almacenar los archivos temporalmente durante la sesi√≥n.

```
local_dir = Path("dog_vision_data")
```

4. **Verificar archivos existentes en Google Drive:**

- Antes de realizar la descarga, se verifica si los archivos ya existen en Google Drive.
- Si est√°n disponibles, se copian al entorno local de Colab para su uso.

5. **Descargar archivos desde la URL de destino**: Si los archivos no existen en Google Drive, se descargan directamente desde la URL proporcionada utilizando el comando `wget`.

6. **Asegurar que el directorio de Google Drive exista:** Se crea la carpeta de destino en Google Drive si a√∫n no existe, para evitar errores al copiar los archivos m√°s tarde.

7. **Copiar los archivos descargados a Google Drive:** Despu√©s de descargarlos, los archivos se guardan en Google Drive para garantizar que est√©n disponibles en futuras sesiones y no sea necesario volver a descargarlos.

> ‚úçüèº **Nota: Descarga local**. Si prefieres ejecutar este proyecto de forma local en lugar de Google Colab, puedes modificar el c√≥digo anterior para guardar los archivos en un directorio local en tu m√°quina en lugar de en Google Drive.

```
# Directorio local para guardar archivos
BASE_DIR = './data/'
os.makedirs(BASE_DIR, exist_ok=True)
```

## Exploraci√≥n de los Datos

La exploraci√≥n de datos es un paso crucial en cualquier proyecto de machine learning o deep learning. Antes de construir un modelo, es importante analizar y entender el conjunto de datos con el que trabajar√°s. Este paso ayuda a identificar posibles problemas, comprender las distribuciones y familiarizarse con las caracter√≠sticas del dataset.

### Objetivos de la exploraci√≥n de datos

- Obtener una **impresi√≥n general** de los datos.
- **Visualizar muestras** para identificar patrones, inconsistencias o anomal√≠as.
- Evaluar las **distribuciones de clases** y las estad√≠sticas clave.
- **Detectar problemas potenciales**, como clases desbalanceadas o datos mal etiquetados.

### Pasos clave para explorar un conjunto de datos

#### 1. Revisar muestras aleatorias

Es fundamental inspeccionar al menos 100 muestras aleatorias para obtener una "impresi√≥n general" del conjunto de datos. Esto incluye:

- **Im√°genes:** Muestra im√°genes aleatorias para entender su calidad, resoluci√≥n y contenido.
- **Textos:** Si trabajas con datos textuales, revisa muestras aleatorias para verificar su estructura y contenido. ¬øLos textos son legibles? ¬øHay caracteres no deseados?
- **Audio:** Si el dataset contiene audio, escucha muestras para verificar su calidad y duraci√≥n.

Esta pr√°ctica no solo te familiariza con los datos, sino que tambi√©n te ayuda a identificar inconsistencias o problemas de etiquetado.

#### 2. Visualizar, visualizar, visualizar

La visualizaci√≥n es clave para la exploraci√≥n de datos. Aunque es √∫til calcular estad√≠sticas, ver los datos directamente puede revelar patrones o problemas que los n√∫meros no muestran.

- Para datasets de **im√°genes**, visualiza un lote de im√°genes con sus etiquetas correspondientes.
- Para datos de **texto**, imprime ejemplos de cada categor√≠a.
- Para datos **tabulares**, utiliza histogramas, diagramas de caja (boxplots) o gr√°ficos de dispersi√≥n para explorar las relaciones entre caracter√≠sticas.

Ejemplo:

```python
import pandas as pd

# Visualizar distribuci√≥n de clases
df = pd.read_csv("labels.csv")  # Archivo de etiquetas
df["class_name"].value_counts().plot(kind="bar", figsize=(10, 5), title="Distribuci√≥n de clases")
```

#### 3. Analizar distribuciones y estad√≠sticas

Eval√∫a m√©tricas clave del dataset, como:

- **Cantidad de muestras:** ¬øCu√°ntas im√°genes/textos/audios hay en total?
- **N√∫mero de clases:** En un problema de clasificaci√≥n, ¬øcu√°ntas clases existen? ¬øHay clases desbalanceadas?
- **Tama√±o y resoluci√≥n de las im√°genes:** ¬øSon consistentes o hay variaciones significativas?
- **Distribuci√≥n de etiquetas por clase:** Identifica si algunas clases tienen muchas m√°s muestras que otras. Las clases desbalanceadas pueden requerir t√©cnicas como sobremuestreo, submuestreo o ajustes en las m√©tricas de evaluaci√≥n.

Ejemplo para verificar la distribuci√≥n de etiquetas:

```python
# Contar el n√∫mero de muestras por clase
print(df["class_name"].value_counts())
```

### Formato objetivo de los datos

Un formato com√∫n para problemas de clasificaci√≥n es organizar los datos en carpetas, donde:

- Cada carpeta representa una clase.
- Las im√°genes correspondientes a cada clase se almacenan dentro de su carpeta.

De esta forma tenemos una estructura clara que asocia cada muestra con su clase correspondiente.

**Ejemplo de formato de carpetas:**

```
dog_vision_data/
‚îú‚îÄ‚îÄ Golden_Retriever/
‚îÇ   ‚îú‚îÄ‚îÄ img1.jpg
‚îÇ   ‚îú‚îÄ‚îÄ img2.jpg
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ Beagle/
‚îÇ   ‚îú‚îÄ‚îÄ img1.jpg
‚îÇ   ‚îú‚îÄ‚îÄ img2.jpg
‚îÇ   ‚îî‚îÄ‚îÄ ...
```

> [!NOTE]
> Esta estructura de formato de carpetas no solo funciona para im√°genes, sino que tambi√©n puede usarse para texto, audio y otros tipos de datos de clasificaci√≥n.

Este formato es ideal para el aprendizaje profundo, ya que:

- **TensorFlow** y otras bibliotecas pueden leer autom√°ticamente esta estructura para asignar etiquetas a cada clase.
- Facilita la generaci√≥n de conjuntos de entrenamiento, validaci√≥n y prueba.

> Como dijo Abraham Lossfunction (una adaptaci√≥n del [famoso consejo de Abraham Lincoln](https://www.brainyquote.com/quotes/abraham_lincoln_109275)):  
> _"Si tuviera 8 horas para construir un modelo de machine learning, dedicar√≠a las primeras 6 horas a preparar mi conjunto de datos."_

### Archivos de listas (`.mat`)

El dataset que utilizaremos incluye archivos `.mat` como `train_list.mat`, `test_list.mat` y `full_list.mat`. Estos archivos contienen informaci√≥n sobre las divisiones de los datos (entrenamiento y prueba) y la organizaci√≥n del conjunto de datos.

**¬øQu√© es un archivo `.mat`?\***

- Un [archivo `.mat`](https://www.mathworks.com/help/matlab/import_export/mat-file-versions.html) es un formato utilizado por MATLAB para almacenar datos.
- Antes de que Python se popularizara en el √°mbito de machine learning y deep learning, [MATLAB](https://www.mathworks.com/products/matlab.html) era ampliamente utilizado para construir modelos y gestionar datasets.

**Abrir archivos `.mat` en Python**
Aunque los archivos `.mat` son nativos de MATLAB, pueden abrirse f√°cilmente en Python utilizando la biblioteca `scipy`, que ya est√° preinstalada en Google Colab.

- **M√©todo para cargar archivos `.mat`:**

  ```python
  from scipy.io import loadmat

  # Cargar un archivo .mat
  mat_data = loadmat("train_list.mat")

  # Explorar el contenido
  print(mat_data.keys())  # Ver las claves disponibles en el archivo .mat
  ```

- **Uso pr√°ctico:**
  Una vez cargados los archivos `.mat`, podemos inspeccionar su contenido para entender c√≥mo est√°n organizadas las muestras. Por ejemplo:
  ```python
  # Ver las primeras entradas del archivo .mat
  train_list = mat_data['train_list']
  print(f"Entradas en la lista de entrenamiento: {len(train_list)}")
  ```

## Procesamiento de Datos

El procesamiento de datos es esencial para preparar la informaci√≥n antes de entrenar un modelo.

### Cargando etiquetas de datos

Las etiquetas son necesarias en problemas supervisados para indicar la clase o valor objetivo.

```python
import pandas as pd
labels = pd.read_csv('labels.csv')
```

### Preparando im√°genes

Las im√°genes deben transformarse en tensores para ser procesadas por los modelos.

```python
from tensorflow.keras.preprocessing.image import ImageDataGenerator
datagen = ImageDataGenerator(rescale=1./255)
```

### Transformando etiquetas en n√∫meros

Se utilizan codificadores como `LabelEncoder` para convertir etiquetas categ√≥ricas en valores num√©ricos.

```python
from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()
encoded_labels = encoder.fit_transform(labels)
```

### Creando un conjunto de validaci√≥n:

Es importante dividir los datos en conjuntos de entrenamiento y validaci√≥n para evaluar el rendimiento del modelo.

```python
from sklearn.model_selection import train_test_split
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)
```

---

## Procesamiento de Im√°genes

El preprocesamiento de im√°genes incluye tareas como normalizaci√≥n y transformaci√≥n en lotes.

### Transformando datos en batches:

Los lotes (batches) permiten procesar conjuntos m√°s peque√±os de datos durante el entrenamiento.

```python
train_data = datagen.flow_from_directory('train/', batch_size=32)
```

### Visualizando datos:

Es fundamental visualizar los datos para verificar su integridad.

```python
import matplotlib.pyplot as plt
plt.imshow(sample_image)
```

---

## Construyendo un Modelo de Deep Learning

### Preparar inputs y outputs:

Los datos de entrada deben normalizarse y las etiquetas deben estar en formato `one-hot` para problemas de clasificaci√≥n.

### Definir el modelo:

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

model = Sequential([
    Dense(128, activation='relu', input_shape=(input_dim,)),
    Dense(num_classes, activation='softmax')
])
```

### Resumen del modelo:

```python
model.summary()
```

---

## Entrenamiento y Evaluaci√≥n

El entrenamiento del modelo implica ajustar los pesos de la red para minimizar el error.

### Entrenar el modelo:

```python
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10)
```

### Evaluar el modelo:

```python
loss, accuracy = model.evaluate(X_val, y_val)
print(f"Loss: {loss}, Accuracy: {accuracy}")
```

### Evitar overfitting:

Se pueden utilizar t√©cnicas como `dropout` y `early stopping`.

```python
from tensorflow.keras.callbacks import EarlyStopping
early_stopping = EarlyStopping(monitor='val_loss', patience=5)
```

---

## Predicciones y Visualizaci√≥n

### Hacer predicciones:

```python
predictions = model.predict(X_test)
```

### Transformar predicciones a texto:

```python
predicted_labels = encoder.inverse_transform(predictions.argmax(axis=1))
```

### Visualizar predicciones:

Mostrar im√°genes junto con las etiquetas predichas.

```python
for i in range(10):
    plt.imshow(X_test[i])
    plt.title(predicted_labels[i])
    plt.show()
```

---

## Guardar y Cargar Modelos

### Guardar el modelo:

```python
model.save('model.h5')
```

### Cargar el modelo:

```python
from tensorflow.keras.models import load_model
model = load_model('model.h5')
```

---

## Ajustar el modelo al dataset completo

Una vez validado el modelo, se puede ajustar utilizando todo el conjunto de datos.

```python
model.fit(X, y, epochs=20)
```

---

## Enviar el Modelo a Kaggle

Exportar predicciones en un formato compatible con Kaggle.

```python
submission.to_csv('submission.csv', index=False)
```

---

- ¬øQu√© es TensorFlow?
- ¬øPor qu√© TensorFlow?
  - write fast deep learning code in python (able to run on a GPU)
  - able to access many pre-built deep learning models
  - whole stack: preprocess, model, deploy
  - originally designed and used in-house by google (now open-source)
- ¬øQu√© es una GPU?
- eligiendo un modelo (throwback)
  - problema 1 (structured) -> model 1 -> structured data (catboost, xgboost, random forest)
  - problema 2 (unstructured) -> model 2 -> unstructured data (deep learning, transfer learning, tensorflow hub)
- ¬øQu√© es deep learning?
- ¬øQu√© son las redes neuronales?
- Google colab:
  - setting up
  - workspace
  - uploading project data
  - setting up our data
  - usando una GPU
  - GPU y google colab
- cargando nuestras etiquetas de datos
- preparando las im√°genes
- transformando data labels en n√∫meros
- creando nuestro conjunto de validaci√≥n
- procesamiento de imagenes
- transformando datos en batches
- visualizando nuestros datos
- preparar nuestros inputs y outputs
- construyendo un modelo de deep learning
- summarizing our model
- evaluando nuestro modelo
- preventing overfitting
- trainning your deep neural network
- make and transform predictions
- transformar predicciones a texto
- visualizando las predicciones del modelo
- visualizando y evaluando las predicciones del modelo
- guardar y cargar el modelo entrenado
- ajustar el modelo a todo el dataset
- hacer predicciones en imagenes de prueba
- submitting model to kaggle
- hacer predicciones con nuestras im√°genes
